#!/usr/bin/env python3

"""
Execute work received from queue

Not using Threads. Not efficiently implemented in python3 and
every worker requires own connection to database. Nothing shared
anyways to warrant threads.
"""

import os
import multiprocessing
import time
import pika
from pika.exceptions import AMQPConnectionError, AMQPChannelError
import psycopg2
import json
import argparse
import configparser
import logging
from logging.handlers import TimedRotatingFileHandler
from load_function_message import load_function_message as lfm, MessageException
from control_message import control_message as cm, MessageException

logger = logging.getLogger(__name__)
running_functions = {}

def process_message(message):
    def pgconnect():
        return psycopg2.connect(
            host=DBHOST,
            database=DBNAME,
            port=DBPORT,
            user=DBUSER,
            password=DBPASS
        )

    pid = os.getpid()

    cluster_id = message.get("cluster_id")
    load_function_id = message.get("load_function_id")
    override_freq = message.get("override_freq")
    analyze = message.get("analyze")
    prime = message.get("prime")
    logger.debug(f"{pid}: start ({cluster_id}, {load_function_id})")

    try:
        """
        @todo need pre-load test detecting if cluster_id is down 
        """
        pg = pgconnect()
        cur = pg.cursor()
        cur.execute("CALL public.load(%s, %s, %s, %s, %s);",
            (cluster_id, analyze, prime, load_function_id, override_freq)
        )
        pg.commit()

        logger.debug( f"{pid}: done ({cluster_id}, {load_function_id})" )
        pg.close()
        rval = True
        pass

    except psycopg2.Error as e:
        logger.error( f"{pid:>10}: database error: {e}" )
        pass

    finally:
        # Ensure the connection is closed in case of an error
        if 'pg' in locals() and pg is not None:
            pg.close()
        pass

    # Must return serializable result
    return make_work(cluster_id, load_function_id)


def make_work(cluster_id, load_function_id):
    return (cluster_id, load_function_id )


def process_callback(result):
    global running_functions
    logger.debug( f"popping {result} {running_functions}" )
    x = running_functions[result[0]].pop( result[1] )
    logger.debug( f"popped {x}" )


def ack_message(ch, method):
    ch.basic_ack( delivery_tag = method.delivery_tag )

def send_ping_response(queue, msgid):
    try:
        creds = pika.PlainCredentials( RMQ_USER, RMQ_PASS )
        ping_conn = pika.BlockingConnection(
            pika.ConnectionParameters(
                host=RMQ_HOST,
                credentials=creds,
                virtual_host = RMQ_VHOST
            )
        )

        ping_ch = ping_conn.channel()
        ping_ch.queue_declare(
             queue = queue,
             durable = False
        )

        logger.debug( f"sending ping response on queue {queue}" )
        msg = cm()
        msg.ping_ack( msgid )
        ping_ch.basic_publish(
            exchange = '',
            routing_key = queue,
            body = msg.as_str()
        )
        logger.debug( f"sent ping response" )

        ping_ch.close()
        ping_conn.close()
    except AMQPConnectionError as e:
        logger.error(f"Connection error: {e}")
        exit(1)
    except AMQPChannelError as e:
        logger.error(f"Channel error: {e}")
        exit(1)
    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
        exit(1)


def callback(ch, method, properties, body):
    logger.debug( f"Received: {body}" )

    message = json.loads( body )

    if message is None:
        # bad json, ack and return
        logger.warning( "received empty message" )
        ack_message(ch, method)
        return False

    # @todo Could do this better leveraging Message class to determine message type
    # Control message
    if "control" in message:
        msg = cm( body )
        """ ACK, all actions will be done immediately """
        ack_message(ch, method)

        if not msg.validate():
            logger.warning( f"not a valid control message" )
            return False

        elif msg.is_ping():
            logger.debug( f"ping received" )
            if time.time() - float(msg.req_ts()) <= RMQ_TTL:
                # assume sender is waiting for a response, else ignore
                send_ping_response( msg.get("response_queue"), msg.get("msgid") )
                pass
            return True

        elif msg.is_stop():
            messager = msg.req_user()+'@'+msg.req_host()+' at '+str(msg.req_ts())
            logger.info( f"told to stop by {messager}")

            # @todo Wait for all executing threads?
            exit(0)

    else:
        msg = lfm( body )

        if not msg.validate():
            logger.warning( "not a valid load function message" )
            ack_message(ch, method)
            return False

        work = make_work( msg.get_cluster_id(), msg.get_load_function_id() )
        logger.debug( f"work: {work}" )
        logger.debug( f"before: {running_functions}" )

        """
        Could be less complicated but may store additional info in dict later
        """
        if msg.get_cluster_id() not in running_functions:
            # Cluster id not in dict neither is load func id
            running_functions[msg.get_cluster_id()] = {}
            running_functions[msg.get_cluster_id()][msg.get_load_function_id()] = True
        elif msg.get_load_function_id() in running_functions[msg.get_cluster_id()]:
            # Case to prevent duplicate load func's running on same cluster id
            logger.info( f"ignoring, load function {msg.get_load_function_id()} for cluster {msg.get_cluster_id()} already running" )
            ack_message(ch, method)
            return True
        else:
            # Cluster id exists and load func id not present
            running_functions[msg.get_cluster_id()][msg.get_load_function_id()] = True

        logger.debug( f"after: {running_functions}" )

        process_pool.apply_async(
            process_message,
            args=(message,),
            callback=process_callback
        )

        """
        Fire & forget, not thrilled by it. Rather worker process perform ack however
        within the confines and purpose of this script fire & forget is not the end
        of the world. Another message will come soon enough to do the same work.
        Better to ack here than have a backlog of messages with one blocking. 
        """
        ack_message(ch, method)


def main():
    # Loops only on exception
    while True:
        try:
            creds = pika.PlainCredentials( RMQ_USER, RMQ_PASS )
            connection = pika.BlockingConnection(
                pika.ConnectionParameters(
                    host = RMQ_HOST,
                    credentials = creds,
                    virtual_host = RMQ_VHOST
                )
            )

            channel = connection.channel()
            channel.queue_declare(
                queue = JOB_QUEUE,
                durable = True,
                arguments = {'x-max-priority': 10}
            )
            
            channel.basic_qos(
                prefetch_count = MAX_WORKERS
            )

            channel.basic_consume(
                queue = JOB_QUEUE,
                on_message_callback = callback
            )
            
            logger.info("Waiting for messages")
            channel.start_consuming()

        except KeyboardInterrupt:
            logger.info("Stopping consumer...")
            break

        except pika.exceptions.AMQPConnectionError:
            logging.warning("Connection lost. Reconnecting...")
            # @todo Make config driven
            time.sleep(5)
        finally:
            try:
                connection.close()
            except Exception:
                pass

if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description=""
        , formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        "-c", "--config"
        , help="Path to configuration file (required)"
        , type=str
    )

    args = parser.parse_args()

    # configuration
    CONFIG = args.config
    config = configparser.ConfigParser()
    config.read(CONFIG)

    MAX_WORKERS = int( config.get('cbmon_load', 'max_workers') )
    JOB_QUEUE = config.get('cbmon_load', 'job_queue')

    DBNAME = config.get('cbmondb', 'name')
    DBUSER = config.get('cbmondb', 'user')
    DBPASS = config.get('cbmondb', 'pass')
    DBHOST = config.get('cbmondb', 'host')
    DBPORT = int( config.get('cbmondb', 'port') )

    RMQ_HOST = config.get('rabbitmq', 'host')
    RMQ_USER = config.get('rabbitmq', 'user')
    RMQ_PASS = config.get('rabbitmq', 'pass')
    RMQ_TTL  = int(config.get('rabbitmq', 'ttl'))
    RMQ_VHOST = config.get('rabbitmq', 'virtual_host')

    LOG_DIR = config.get('logging', 'dir')
    LOG_LEVEL = config.get('logging', 'level')

    py_name = os.path.basename(__file__)
    LOG_FILE = LOG_DIR + '/' + py_name + '.log'

    logger.setLevel(getattr(logging, LOG_LEVEL))

    # Create a file handler
    file_handler = logging.FileHandler(LOG_FILE)
    file_handler.setLevel(getattr(logging, LOG_LEVEL))
    file_handler = TimedRotatingFileHandler(
        LOG_FILE, when="midnight", interval=1, backupCount=7
    )
    formatter = logging.Formatter('%(asctime)s,' + py_name + ',%(funcName)s(),%(levelname)s,%(message)s', "%Y-%m-%d %H:%M:%S")
    file_handler.setFormatter(formatter)

    # Where logs are going, file & console
    if config.get('logging', 'console') == 'on':
        ch = logging.StreamHandler()
        ch.setFormatter(formatter)
        logger.addHandler(ch)
    logger.addHandler(file_handler)

    # Create a pool of workers
    process_pool = multiprocessing.Pool(
        processes = MAX_WORKERS
    )
    main()
    process_pool.close()
    process_pool.join()


